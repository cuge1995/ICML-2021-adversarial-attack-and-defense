# ICML-2021-adversarial-attack-and-defense

- Towards Defending against Adversarial Examples via Attack-Invariant Features
```
denoise net
```

- Measuring the Effectiveness of Dataset Manipulation Attacks
```
paper not yet
```

- Making Paper Reviewing Robust to Bid Manipulation Attacks
```
code:https://github.com/facebookresearch/secure-paper-bidding
new scene attack
```

- Maximum Mean Discrepancy is Aware of Adversarial Attacks
```
adversarial attack detection
```

- Neural Tangent Generalization Attacks
```
paper not yet
```

- Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm
```
code:https://github.com/VITA-Group/SparseADV_Homotopy
```

- Learning to Generate Noise for Multi-Attack Robustness
```
improve the modelâ€™s robustness against multiple types of attacks
```

- Label Inference Attacks from Log-loss Scores
```
Inference attack
```

- PopSkipJump: Decision-Based Attack for Probabilistic Classifiers
```
code:https://github.com/cjsg/PopSkipJump
```

- Query Complexity of Adversarial Attacks
```
analyse
```

- Robust Testing and Estimation under Manipulation Attacks
```
denoise 
```

- Defense against backdoor attacks via robust covariance estimation
```
paper not yet
```

- Model-Targeted Poisoning Attacks with Provable Convergence
```
Poisoning Attacks
```

- Robust Learning for Data Poisoning Attacks
```
paper not yet
```

- Recovering AES Keys with a Deep Cold Boot Attack
```
Cold boot attacks
```

- Knowledge Enhanced Machine Learning Pipeline against Diverse Adversarial Attacks
```
the prediction of KEMLP is more robust than that of the main DNN model
code:https://github.com/AI-secure/Knowledge-Enhanced-Machine-Learning-Pipeline
```

- Expressive 1-Lipschitz Neural Networks for Robust Multiple Graph Learning against Adversarial Attacks
```
paper not yet
```

- When Does Data Augmentation Help With Membership Inference Attacks?
```
paper not yet
```

- Label-Only Membership Inference Attacks
```
Membership Inference Attacks
code:https://github.com/label-only/membership-inference
```

- CRFL: Certifiably Robust Federated Learning against Backdoor Attacks
```
Certificate defense
code:https://github.com/AI-secure/CRFL
```

- Progressive-Scale Boundary Blackbox Attack via Projective Gradient Estimation
```
 Blackbox Attack
 code:https://github.com/AI-secure/PSBA
```

- Mind the box: l1-APGD for sparse adversarial attacks on image classifiers
```
l1-APGD
```
